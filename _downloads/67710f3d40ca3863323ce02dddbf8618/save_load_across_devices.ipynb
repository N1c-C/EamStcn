{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8gy-nsAsgij"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMa1rKjjsgim"
      },
      "source": [
        "\n",
        "# Saving and loading models across devices in PyTorch\n",
        "\n",
        "There may be instances where you want to save and load your neural\n",
        "networks across different devices.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Saving and loading models across devices is relatively straightforward\n",
        "using PyTorch. In this recipe, we will experiment with saving and\n",
        "loading models across CPUs and GPUs.\n",
        "\n",
        "## Setup\n",
        "\n",
        "In order for every code block to run properly in this recipe, you must\n",
        "first change the runtime to “GPU” or higher. Once you do, we need to\n",
        "install ``torch`` if it isn’t already available.\n",
        "\n",
        "::\n",
        "\n",
        "   pip install torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qEqBeNXsgio"
      },
      "source": [
        "## Steps\n",
        "\n",
        "1. Import all necessary libraries for loading our data\n",
        "2. Define and intialize the neural network\n",
        "3. Save on a GPU, load on a CPU\n",
        "4. Save on a GPU, load on a GPU\n",
        "5. Save on a CPU, load on a GPU\n",
        "6. Saving and loading ``DataParallel`` models\n",
        "\n",
        "### 1. Import necessary libraries for loading our data\n",
        "\n",
        "For this recipe, we will use ``torch`` and its subsidiaries ``torch.nn``\n",
        "and ``torch.optim``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS2XfeSisgio"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UjyO5Uvsgip"
      },
      "source": [
        "### 2. Define and intialize the neural network\n",
        "\n",
        "For sake of example, we will create a neural network for training\n",
        "images. To learn more see the Defining a Neural Network recipe.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wELjpLaxsgip"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyYdm83ksgiq"
      },
      "source": [
        "### 3. Save on GPU, Load on CPU\n",
        "\n",
        "When loading a model on a CPU that was trained with a GPU, pass\n",
        "``torch.device('cpu')`` to the ``map_location`` argument in the\n",
        "``torch.load()`` function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hazyixdpsgiq"
      },
      "outputs": [],
      "source": [
        "# Specify a path to save to\n",
        "PATH = \"model.pt\"\n",
        "\n",
        "# Save\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "device = torch.device('cpu')\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load(PATH, map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js5wH1BAsgir"
      },
      "source": [
        "In this case, the storages underlying the tensors are dynamically\n",
        "remapped to the CPU device using the ``map_location`` argument.\n",
        "\n",
        "### 4. Save on GPU, Load on GPU\n",
        "\n",
        "When loading a model on a GPU that was trained and saved on GPU, simply\n",
        "convert the initialized model to a CUDA optimized model using\n",
        "``model.to(torch.device('cuda'))``.\n",
        "\n",
        "Be sure to use the ``.to(torch.device('cuda'))`` function on all model\n",
        "inputs to prepare the data for the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HlCGLWLsgis"
      },
      "outputs": [],
      "source": [
        "# Save\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "device = torch.device(\"cuda\")\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fzx-FOIsgis"
      },
      "source": [
        "Note that calling ``my_tensor.to(device)`` returns a new copy of\n",
        "``my_tensor`` on GPU. It does NOT overwrite ``my_tensor``. Therefore,\n",
        "remember to manually overwrite tensors:\n",
        "``my_tensor = my_tensor.to(torch.device('cuda'))``.\n",
        "\n",
        "### 5. Save on CPU, Load on GPU\n",
        "\n",
        "When loading a model on a GPU that was trained and saved on CPU, set the\n",
        "``map_location`` argument in the ``torch.load()`` function to\n",
        "``cuda:device_id``. This loads the model to a given GPU device.\n",
        "\n",
        "Be sure to call ``model.to(torch.device('cuda'))`` to convert the\n",
        "model’s parameter tensors to CUDA tensors.\n",
        "\n",
        "Finally, also be sure to use the ``.to(torch.device('cuda'))`` function\n",
        "on all model inputs to prepare the data for the CUDA optimized model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1T13nR4sgis"
      },
      "outputs": [],
      "source": [
        "# Save\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "device = torch.device(\"cuda\")\n",
        "model = Net()\n",
        "# Choose whatever GPU device number you want\n",
        "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
        "# Make sure to call input = input.to(device) on any input tensors that you feed to the model\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKnId94fsgit"
      },
      "source": [
        "### 6. Saving ``torch.nn.DataParallel`` Models\n",
        "\n",
        "``torch.nn.DataParallel`` is a model wrapper that enables parallel GPU\n",
        "utilization.\n",
        "\n",
        "To save a ``DataParallel`` model generically, save the\n",
        "``model.module.state_dict()``. This way, you have the flexibility to\n",
        "load the model any way you want to any device you want.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k7fLaaDsgit"
      },
      "outputs": [],
      "source": [
        "# Save\n",
        "torch.save(net.module.state_dict(), PATH)\n",
        "\n",
        "# Load to whatever device you want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn8XDK1Esgiv"
      },
      "source": [
        "Congratulations! You have successfully saved and loaded models across\n",
        "devices in PyTorch.\n",
        "\n",
        "## Learn More\n",
        "\n",
        "Take a look at these other recipes to continue your learning:\n",
        "\n",
        "-  TBD\n",
        "-  TBD\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}