{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjBeTMLJQJuQ"
   },
   "source": [
    "# Instance Segmentation with Mask Region Based Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUyLYdu9mWN8"
   },
   "source": [
    "The Mask R-CNN has been trained on the coco datset and and there are a number of classes that occur in the Davis 16 training sequences.\n",
    "\n",
    "Bear,\n",
    "Bus,\n",
    "Car,\n",
    "Dog,\n",
    "Elephant,\n",
    "Horse,\n",
    "Motor Cycle,\n",
    "Kite and\n",
    "Train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKDPvqlimWN9"
   },
   "source": [
    "\n",
    "\n",
    "The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes.\n",
    "\n",
    "\n",
    "During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows:\n",
    "\n",
    "- boxes (Tensor[N, 4]): the predicted boxes in [x0, y0, x1, y1] format, with values between 0 and H and 0 and W\n",
    "\n",
    "- labels (Tensor[N]): the predicted labels for each image\n",
    "\n",
    "-  scores (Tensor[N]): the scores or each prediction\n",
    "\n",
    "-  masks (Tensor[N, H, W]): the predicted masks for each instance, in 0-1 range. In order to obtain the final segmentation masks, the soft masks can be thresholded, generally with a value of 0.5 (mask >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3161,
     "status": "ok",
     "timestamp": 1664130797641,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "CLEfxNKK1OC9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Papa/opt/anaconda3/envs/Efficient_Adaptive_Mem_STM1/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from typing import Any, Callable, cast, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "from skimage.morphology import disk\n",
    "\n",
    "\n",
    "\n",
    " # These are the classes that are available in the COCO-Dataset\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-j36eELmWN-"
   },
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a03a348d967041b1a8c96a31fb8800df",
      "e1923b29c4a34d4db439bfb6667809d4",
      "233167bf251b4bae91ea1dfdf1702ce1",
      "07eddc2d5b7a434891b54b8af0029b30",
      "7cd366cb4d3841f0bb8d9bc05fd3895c",
      "9edcf0c51d2e447895cbe211916550be",
      "94661c6de4194b599c128bb80a5b258f",
      "a0233626d1c0440dbe10e2743a85ee2e",
      "13cd0360e0dd4e698ac36a351dfe92be",
      "116936e1eb704b7c8f4f4602f3d4d5e7",
      "b2e001a869244d2db6ec4faf14a774cb"
     ]
    },
    "executionInfo": {
     "elapsed": 9795,
     "status": "ok",
     "timestamp": 1664130807429,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "CISYQjUFmWN-",
    "outputId": "39ebd0db-902f-4858-b34c-ce28fcc8f0d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /Users/Papa/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
      "1.3%"
     ]
    }
   ],
   "source": [
    "# get the pretrained model from torchvision.models\n",
    "# Note: pretrained=True will get the pretrained weights for the model.\n",
    "# model.eval() to use the model for inference\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1664130807431,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "3AMhM4Uz1sd9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1664130807431,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "KCuzRQhm12Xk"
   },
   "outputs": [],
   "source": [
    "# We will use the following colors to fill the pixels\n",
    "colours = [[0, 255, 0],\n",
    "           [0, 0, 255],\n",
    "           [255, 0, 0],\n",
    "           [0, 255, 255],\n",
    "           [255, 255, 0],\n",
    "           [255, 0, 255],\n",
    "           [80, 70, 180],\n",
    "           [250, 80, 190],\n",
    "           [245, 145, 50],\n",
    "           [70, 150, 250],\n",
    "           [50, 190, 190]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1664130807432,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "-swPEpDuMIML"
   },
   "outputs": [],
   "source": [
    "\"\"\" Utilities for computing, reading and saving DAVIS benchmark evaluation.\"\"\"\n",
    "\n",
    "def get_contours(mask, bound_th=0.5):\n",
    "    \"\"\" Returns one pixel wide contours of mask as a bit ndarray\n",
    "    :param mask: A ground truth or predicted 2D (grayscale) image mask (H x W )\n",
    "    :param bound_th: The pixel threshold value for a one or zero assignment\n",
    "    \"\"\"\n",
    "\n",
    "    # Tests the mask is only 2 dimensional H X W. Raise AssertionError otherwise\n",
    "    assert len(mask.shape) == 2, f\"Mask should be 2D (HxW) but got {mask.shape}\"\n",
    "    # Convert image to true binary based on the threshold and find contours - all points\n",
    "    ret, thresh = cv.threshold(mask, int(bound_th * 1), 1, 0)\n",
    "    # Set approximation as CHAIN_APPROX_NONE to keep all points (at slower speed)\n",
    "    contours = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    # Create a blank canvas the same shape as the input mask and overlay the contours\n",
    "    cont_matte = np.zeros_like(mask)\n",
    "    for x in contours:\n",
    "        for arr in x:\n",
    "            cont_matte[arr[0][1], arr[0][0]] = 1\n",
    "\n",
    "    # plt.imshow(cont_matte, cmap='gray')\n",
    "    # plt.show()\n",
    "    return cont_matte\n",
    "def eval_boundary(foreground_mask, gt_mask, bound_th=0.008):\n",
    "    \"\"\"\n",
    "    Compute mean,recall and decay from per-frame evaluation.\n",
    "    Calculates precision/recall for boundaries between foreground_mask and\n",
    "    gt_mask.\n",
    "    :param foreground_mask: (ndarray) binary segmentation image.\n",
    "    :param gt_mask:         (ndarray): binary annotated image.\n",
    "    Returns:\n",
    "        F (float): boundaries F-measure\n",
    "        P (float): boundaries precision\n",
    "        R (float): boundaries recall\n",
    "\n",
    "    Based on github fperazzi/davis but using openCV functions for finding contours\n",
    "    and dilation to significantly improve speed\n",
    "    \"\"\"\n",
    "    # tests the mask is only 2 dimensional H X W. Raise AssertionError otherwise\n",
    "    assert len(foreground_mask.shape) == 2, f\"Foreground mask should be 2D (HxW) but got {foreground_mask.shape}\"\n",
    "\n",
    "    # Get the pixel boundaries of both masks\n",
    "    fg_boundary = get_contours(foreground_mask, bound_th=0.1)\n",
    "    gt_boundary = get_contours(gt_mask)\n",
    "\n",
    "    # Get a disk radius proportional to the size of image and\n",
    "    # dilate contours proportionally\n",
    "    bound_pix = bound_th if bound_th >= 1 else \\\n",
    "        np.ceil(bound_th * np.linalg.norm(foreground_mask.shape))\n",
    "    fg_dil = cv.dilate(fg_boundary, disk(bound_pix))\n",
    "    gt_dil = cv.dilate(gt_boundary, disk(bound_pix))\n",
    "\n",
    "    # Get the intersection\n",
    "    gt_match = gt_boundary * fg_dil\n",
    "    fg_match = fg_boundary * gt_dil\n",
    "\n",
    "    # Area of the intersection\n",
    "    n_fg = np.sum(fg_boundary)\n",
    "    n_gt = np.sum(gt_boundary)\n",
    "\n",
    "    # % Compute precision and recall\n",
    "    if n_fg == 0 and n_gt > 0:\n",
    "        precision = 1\n",
    "        recall = 0\n",
    "    elif n_fg > 0 and n_gt == 0:\n",
    "        precision = 0\n",
    "        recall = 1\n",
    "    elif n_fg == 0 and n_gt == 0:\n",
    "        precision = 1\n",
    "        recall = 1\n",
    "    else:\n",
    "        precision = np.sum(fg_match) / float(n_fg)\n",
    "        recall = np.sum(gt_match) / float(n_gt)\n",
    "\n",
    "    # Compute F measure\n",
    "    if precision + recall == 0:\n",
    "        F = 0\n",
    "    else:\n",
    "        F = 2 * precision * recall / (precision + recall)\n",
    "    return F, precision, recall\n",
    "\n",
    "\n",
    "def eval_iou(foreground_mask, gt_mask):\n",
    "\n",
    "    \"\"\" Compute region similarity (intersection over union IoU as the Jaccard Index.\n",
    "    As github fperazzi/davis but with np.bool operations removed as they have been dropped.\n",
    "    Mask values should be 0 or 1\n",
    "    :params foreground_mask (ndarray): Predicted 2D binary annotation mask - HxW of form cv.CV_8UC1.\n",
    "            gt_mask (ndarray): Provided 2D binary annotation mask - HxW of form cv.CV_8UC1.\n",
    "    :returns jaccard (float): region similarity\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # test for a blank image  and avoid div by 0 err\n",
    "    if np.isclose(np.sum(gt_mask), 0) and np.isclose(np.sum(foreground_mask), 0):\n",
    "        return 1\n",
    "    else:\n",
    "        # &, |  elementwise operators\n",
    "        return np.sum((gt_mask & foreground_mask)) / np.sum((gt_mask | foreground_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89flam1cPf43"
   },
   "source": [
    "## Running inference on an sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1664130807433,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "fagQA5B6224N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01gEGATmns7"
   },
   "source": [
    "# Set up test dataset and datloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1664130807433,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "COFSkYJa6lKD"
   },
   "outputs": [],
   "source": [
    "# class ImglistToTensor(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Converts a list of PIL images in the range [0,255] to a torch.FloatTensor\n",
    "#     of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1].\n",
    "#     Can be used as first transform for ``VideoFrameDataset``.\n",
    "#     \"\"\"\n",
    "#     @staticmethod\n",
    "#     def forward(img_list: List[Image.Image]) -> 'torch.Tensor[NUM_IMAGES, CHANNELS, HEIGHT, WIDTH]':\n",
    "#         \"\"\"\n",
    "#         Converts each PIL image in a list to\n",
    "#         a torch Tensor and stacks them into\n",
    "#         a single tensor.\n",
    "#         Args:\n",
    "#             img_list: list of PIL images.\n",
    "#         Returns:\n",
    "#             tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n",
    "#         \"\"\"\n",
    "#         return torch.stack([transforms.functional.to_tensor(pic) for pic in img_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1664130807434,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "a3g6XRE59xIl"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
    "from torchvision.datasets.folder import default_loader, has_file_allowed_extension\n",
    "from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class SelectImageFolder(torchvision.datasets.DatasetFolder):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            chosen_classes: list,\n",
    "            extensions: Optional[Tuple[str, ...]] = None,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            loader: Callable[[str], Any] = default_loader,\n",
    "            is_valid_file: Optional[Callable[[str], bool]] = None,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.extensions = IMG_EXTENSIONS if is_valid_file is None else None\n",
    "\n",
    "        classes, class_to_idx = self.find_classes(self.root, chosen_classes)\n",
    "        samples = self.make_dataset(self.root, class_to_idx, self.extensions, is_valid_file)\n",
    "\n",
    "        self.loader = loader\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.samples = samples\n",
    "        self.targets = [s[1] for s in samples]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dataset(\n",
    "            directory: str,\n",
    "            class_to_idx: Dict[str, int],\n",
    "            extensions: Optional[Tuple[str, ...]] = None,\n",
    "            is_valid_file: Optional[Callable[[str], bool]] = None,\n",
    "    ) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Generates a list of samples of a form (path_to_sample, class).\n",
    "            Overwrites parent function - See :class:`DatasetFolder` for details\"\"\"\n",
    "\n",
    "        if class_to_idx is None:\n",
    "            raise ValueError(\"The class_to_idx parameter cannot be None.\")\n",
    "        return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)\n",
    "\n",
    "    def find_classes(self, directory: str, chosen_classes: list) -> Tuple[List[str], Dict[str, int]]:\n",
    "        \"\"\"Find the class folders in a dataset folder structure\n",
    "        Overwrites parent function - See :class:`DatasetFolder` for details\"\"\"\n",
    "        return find_classes(directory, chosen_classes)\n",
    "\n",
    "\n",
    "def find_classes(directory: str, chosen_classes: list):\n",
    "    \"\"\"Finds the class folders in a dataset. This is an over load function to\n",
    "    to allow for customisation\n",
    "    Tuple[List[str], Dict[str, int]\n",
    "  See :class:`DatasetFolder` for details. \n",
    "  \"\"\"\n",
    "    classes = sorted(\n",
    "        entry.name for entry in os.scandir(directory) if entry.is_dir() and entry.name in chosen_classes)\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(directory: str,\n",
    "                 class_to_idx: Optional[Dict[str, int]] = None,\n",
    "                 extensions: Optional[Union[str, Tuple[str, ...]]] = None,\n",
    "                 is_valid_file: Optional[Callable[[str], bool]] = None,\n",
    "                 ) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Generates a list of samples of a form (path_to_sample, class).\n",
    "    See :class:`DatasetFolder` for details.\n",
    "    Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function\n",
    "    by default.\n",
    "    \"\"\"\n",
    "    directory = os.path.expanduser(directory)\n",
    "\n",
    "    if class_to_idx is None:\n",
    "        _, class_to_idx = find_classes(directory)\n",
    "    elif not class_to_idx:\n",
    "        raise ValueError(\"'class_to_index' must have at least one entry to collect any samples.\")\n",
    "\n",
    "    both_none = extensions is None and is_valid_file is None\n",
    "    both_something = extensions is not None and is_valid_file is not None\n",
    "    if both_none or both_something:\n",
    "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
    "\n",
    "    if extensions is not None:\n",
    "        def is_valid_file(x: str) -> bool:\n",
    "            return has_file_allowed_extension(x, extensions)  # type: ignore[arg-type]\n",
    "\n",
    "    is_valid_file = cast(Callable[[str], bool], is_valid_file)\n",
    "\n",
    "    instances = []\n",
    "    available_classes = set()\n",
    "    for target_class in sorted(class_to_idx.keys()):\n",
    "        class_index = class_to_idx[target_class]\n",
    "        target_dir = os.path.join(directory, target_class)\n",
    "        if not os.path.isdir(target_dir):\n",
    "            continue\n",
    "        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = os.path.join(root, fname)\n",
    "                if is_valid_file(path):\n",
    "                    item = path, class_index\n",
    "                    instances.append(item)\n",
    "\n",
    "                    if target_class not in available_classes:\n",
    "                        available_classes.add(target_class)\n",
    "\n",
    "    empty_classes = set(class_to_idx.keys()) - available_classes\n",
    "    if empty_classes:\n",
    "        msg = f\"Found no valid file for the classes {', '.join(sorted(empty_classes))}. \"\n",
    "        if extensions is not None:\n",
    "            msg += f\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\n",
    "        raise FileNotFoundError(msg)\n",
    "\n",
    "    return instances\n",
    "\n",
    "# find_classes(PROJECT_ROOT_DIR + \"JPEGImages/480p/\", 'bear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37055,
     "status": "ok",
     "timestamp": 1664130844462,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "r3zAk7Knmns7",
    "outputId": "9cd2e5a6-673c-4c2d-ca2c-b31e72307c6c"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# If Notebook is running in Colab change location of images to google drive\n",
    "\n",
    "MULTI_PROCESSOR = False\n",
    "WORKERS = 1\n",
    "ON_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT_DIR = \"drive/MyDrive/DAVIS/DAVIS/\"\n",
    "    MULTI_PROCESSOR = True\n",
    "    WORKERS = 8\n",
    "else:\n",
    "    PROJECT_ROOT_DIR = \".\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "executionInfo": {
     "elapsed": 1243189,
     "status": "error",
     "timestamp": 1664132087644,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "KwhyJqX-eXaN",
    "outputId": "d8a38682-3b98-42cd-a64d-ec9d4d051577"
   },
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def label_cnt(targets, classes):\n",
    "  \"\"\" Given the set of labels for a Torch data loader,\n",
    "  returns the number of each label as a print string\"\"\"\n",
    "  label_cnt = {}\n",
    "  for item in targets:\n",
    "    label_cnt[classes[item]] = label_cnt.get(classes[item],0) + 1\n",
    "  cnt = ''\n",
    "  for key,value in label_cnt.items():\n",
    "    cnt += f\"{str(key)}: {str(value)}, \"\n",
    "  return cnt\n",
    "\n",
    "# Transform data to fixed size, tensor and normalised to the ImageNet means\n",
    "seq_folder =  ['bear','bus','car-roundabout','car-turn','dog','dog-agility',\n",
    "                  'drift-turn','horsejump-low', 'kite-surf', 'motor-bike', 'train']\n",
    "# seq_folder =  ['bear']                 \n",
    "def_transforms = transforms.Compose([\n",
    "    # transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize(mean = [0.485,0.456,0.406], std = [0.229,0.224,0.225])\n",
    "    # scalar\n",
    "])\n",
    "\n",
    "gt_transforms = transforms.Compose([\n",
    "    # transforms.Resize(IMG_SIZE),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize(mean = [0.485,0.456,0.406], std = [0.229,0.224,0.225])\n",
    "    # scalar\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "test_data_path = PROJECT_ROOT_DIR + \"JPEGImages/480p/\"\n",
    "gt_mask_path = PROJECT_ROOT_DIR + \"Annotations/480p/\"\n",
    "results = {}\n",
    "for seq in seq_folder:\n",
    "  results[seq]={'f':[], 'j':[]}\n",
    "  # Adjust to alter mini Batch size\n",
    "  batch_size = 32\n",
    "\n",
    "  # Define test data  and gt_mask data sets \n",
    "  test_data = SelectImageFolder(\n",
    "      root = test_data_path,\n",
    "      chosen_classes = seq,\n",
    "      transform = def_transforms\n",
    "  )\n",
    "  gt_mask_data = SelectImageFolder(\n",
    "      root=gt_mask_path,\n",
    "      chosen_classes = seq,\n",
    "      transform = gt_transforms\n",
    "  )\n",
    "\n",
    "  # Create train, and truth data loaders\n",
    "  test_loader = data.DataLoader(test_data, batch_size = 1,shuffle=False)\n",
    "  gt_mask_loader = data.DataLoader(gt_mask_data, batch_size = 1,shuffle=False)\n",
    "  print(f'{len(test_loader.dataset)} test samples.{label_cnt(test_data.targets, test_data.classes)}')\n",
    "  print(f'{len(gt_mask_loader.dataset)} annotation.{label_cnt(gt_mask_data.targets, gt_mask_data.classes)}')\n",
    "  # loop through each test sequence and measure metrics\n",
    "  test_ex = iter(test_loader)\n",
    "  truth = iter(gt_mask_loader)\n",
    "  for idx in range(len(test_loader)):\n",
    "    # test_batch = next(test_ex)\n",
    "    # gt_mask_batch = next(truth)\n",
    "\n",
    "    frames, targets= test_ex.next()\n",
    "    gt_masks, _ = truth.next()\n",
    "    # frames.to(device)\n",
    "    # targets.to(device)\n",
    "\n",
    "    \n",
    "    # Inference \n",
    "    if torch.cuda.is_available():\n",
    "        preds = model(frames.cuda())\n",
    "        # Determine the most likely mask index\n",
    "        max_score_idx = np.argmax(preds[0]['scores'][0].detach().cpu().numpy())\n",
    "\n",
    "        # calculate the measurements and add to dict\n",
    "        mask = (preds[0]['masks'][max_score_idx]>0.5).squeeze().detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        # frame = (frames.squeeze().detach().permute(1,2,0).numpy())\n",
    "        gt_mask = (gt_masks.squeeze().detach().cpu().numpy()).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        preds = model(frames)\n",
    "      \n",
    "        # Determine the most likely mask index\n",
    "        max_score_idx = np.argmax(preds[0]['scores'][0].detach().numpy())\n",
    "\n",
    "        # calculate the measurements and add to dict\n",
    "        mask = (preds[0]['masks'][max_score_idx]>0.5).squeeze().detach().numpy().astype(np.uint8)\n",
    "\n",
    "        # frame = (frames.squeeze().detach().permute(1,2,0).numpy())\n",
    "        gt_mask = (gt_masks.squeeze().detach().numpy()).astype(np.uint8)\n",
    "        \n",
    "        # plt.imshow(frame)\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.imshow(gt_mask, cmap='gray')\n",
    "        # plt.show()\n",
    "\n",
    "    results[seq]['f'].append(eval_boundary(mask, gt_mask, bound_th=0.008))\n",
    "    results[seq]['j'].append(eval_iou(mask, gt_mask))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1664132087645,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "mycE2tox0LM8"
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1664134165743,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "-240qEvGBQIJ",
    "outputId": "01d438e3-45ed-4e40-fba7-a35b15c22680"
   },
   "outputs": [],
   "source": [
    "# Calculate average IoU & Boundary values \n",
    "print(f'Sequence{\" \": <15s}J{\" \": <10s}F{\" \": <10s}J&F{\" \": <8s}Best Frame{\" \": <8s}Worse frame')\n",
    "for seq, scores in results.items():\n",
    "  samples = len(scores['j'])\n",
    "\n",
    "  print(\n",
    "    f'{seq: <18s}    {sum(scores[\"j\"])/samples:.2f}',\n",
    "    f'      {sum(scores[\"f\"][0])/samples:.2f}',\n",
    "    f'      {(sum(scores[\"j\"])/samples + sum(scores[\"f\"][0])/samples)/2:.2f}',\n",
    "    f'          {np.argmax(scores[\"f\"])}{\"  \": <3s}',\n",
    "    f'\\t\\t{np.argmin(scores[\"f\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-UxMg5UILTD"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1664132087646,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "iTL-OFlWNSWq"
   },
   "outputs": [],
   "source": [
    "# # We will keep only the pixels with values  greater than 0.5 as 1, and set the rest to 0.\n",
    "# print(pred[0]['masks'].shape)\n",
    "# print(pred[0]['masks'].squeeze().shape)\n",
    "# print(pred[0]['masks'].squeeze().detach().shape)\n",
    "# print(pred[0]['masks'].squeeze().detach().numpy().shape)\n",
    "# print(pred[0]['masks'].squeeze().detach().numpy())\n",
    "# masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
    "# print(masks.shape)\n",
    "# print(masks)\n",
    "# print(pred[0]['labels'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1664132087646,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "kXEKjA_fNSZg"
   },
   "outputs": [],
   "source": [
    "# # Let's plot the mask for the `person` class since the 0th mask belongs to `person`\n",
    "# plt.imshow(masks, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1664132087646,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "oGLyq3wY1CeO"
   },
   "outputs": [],
   "source": [
    "# # Let's color the `person` mask using the `random_colour_masks` function\n",
    "# mask1 = random_colour_masks(masks)\n",
    "# plt.imshow(mask1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1664132087647,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "6kH3KPIqWgvg"
   },
   "outputs": [],
   "source": [
    "# # Let's blend the original and the masked image and plot it.\n",
    "# blend_img = cv2.addWeighted(np.asarray(img), 0.5, mask1, 0.5, 0)\n",
    "\n",
    "# plt.imshow(blend_img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1664132087647,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "KiHf0X1_yGuE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fG-zZ86mWN_"
   },
   "source": [
    "Let's create some helper functions.   \n",
    "We will create the `random_colour_masks()` function to fill the predicted-mask with colors, `get_predictions()` to return the final predictions from the model and finally the `instance_segmentation_api()` to overlay the colored mask over the original image and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1664132087649,
     "user": {
      "displayName": "Nic Chant",
      "userId": "07825353379441549817"
     },
     "user_tz": -60
    },
    "id": "r7ISpsTHmWN_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def random_colour_masks(image):\n",
    "    \"\"\"\n",
    "    random_colour_masks\n",
    "    parameters:\n",
    "      - image - predicted masks\n",
    "    method:\n",
    "      - the masks of each predicted object is given random colour for visualization\n",
    "    \"\"\"\n",
    "    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    r[image == 1], g[image == 1], b[image == 1] = colours[random.randrange(0,10)]\n",
    "    coloured_mask = np.stack([r, g, b], axis=2)\n",
    "    return coloured_mask\n",
    "\n",
    "def get_prediction(img_path, threshold):\n",
    "    \"\"\"\n",
    "    get_prediction\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "    method:\n",
    "      - Image is obtained from the image path\n",
    "      - the image is converted to image tensor using PyTorch's Transforms\n",
    "      - image is passed through the model to get the predictions\n",
    "      - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks\n",
    "        ie: eg. segment of cat is made 1 and rest of the image is made 0\n",
    "\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img = transform(img)\n",
    "    pred = model([img])\n",
    "    pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "    print(\"pred_score: \", pred_score)\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n",
    "    print(\"pred_t: \", pred_t)\n",
    "    masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
    "    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "    masks = masks[:pred_t+1]\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    print(pred_boxes)\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    print(pred_class)\n",
    "    return masks, pred_boxes, pred_class\n",
    "\n",
    "\n",
    "def instance_segmentation_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
    "    \"\"\"\n",
    "    instance_segmentation_api\n",
    "    parameters:\n",
    "      - img_path - path to input image\n",
    "    method:\n",
    "      - prediction is obtained by get_prediction\n",
    "      - each mask is given random color\n",
    "      - each mask is added to the image in the ration 1:0.8 with opencv\n",
    "      - final output is displayed\n",
    "    \"\"\"\n",
    "    masks, boxes, pred_cls = get_prediction(img_path, threshold)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    for i in range(len(masks)):\n",
    "        rgb_mask = random_colour_masks(masks[i])\n",
    "        img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
    "        # cv2.rectangle(img, int( boxes[i][0]), int(boxes[i][1]),color=(0, 255, 0), thickness=rect_th)\n",
    "        cv2.rectangle(img, (int( boxes[i][0][0]),int( boxes[i][0][1]) ), \n",
    "                      (int( boxes[i][1][0]),int( boxes[i][1][1]) ),\n",
    "                      color=(0, 255, 0), thickness=rect_th)\n",
    "        cv2.putText(img,pred_cls[i], (int( boxes[i][0][0]),int( boxes[i][0][1]) ),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
    "    plt.figure(figsize=(20,30))\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1StS_58uMcPajZ6Qr9aPvQCl_5waq6-nh",
     "timestamp": 1664028427333
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07eddc2d5b7a434891b54b8af0029b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_116936e1eb704b7c8f4f4602f3d4d5e7",
      "placeholder": "​",
      "style": "IPY_MODEL_b2e001a869244d2db6ec4faf14a774cb",
      "value": " 170M/170M [00:02&lt;00:00, 75.8MB/s]"
     }
    },
    "116936e1eb704b7c8f4f4602f3d4d5e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13cd0360e0dd4e698ac36a351dfe92be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "233167bf251b4bae91ea1dfdf1702ce1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0233626d1c0440dbe10e2743a85ee2e",
      "max": 178090079,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13cd0360e0dd4e698ac36a351dfe92be",
      "value": 178090079
     }
    },
    "7cd366cb4d3841f0bb8d9bc05fd3895c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94661c6de4194b599c128bb80a5b258f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9edcf0c51d2e447895cbe211916550be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0233626d1c0440dbe10e2743a85ee2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a03a348d967041b1a8c96a31fb8800df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e1923b29c4a34d4db439bfb6667809d4",
       "IPY_MODEL_233167bf251b4bae91ea1dfdf1702ce1",
       "IPY_MODEL_07eddc2d5b7a434891b54b8af0029b30"
      ],
      "layout": "IPY_MODEL_7cd366cb4d3841f0bb8d9bc05fd3895c"
     }
    },
    "b2e001a869244d2db6ec4faf14a774cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1923b29c4a34d4db439bfb6667809d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9edcf0c51d2e447895cbe211916550be",
      "placeholder": "​",
      "style": "IPY_MODEL_94661c6de4194b599c128bb80a5b258f",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
